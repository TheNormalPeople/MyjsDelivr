---
 title: 绪论
 categories: 机器学习
 tags： 机器学习
---

# 1.1 引言

* 机器学习所研究的主要内容，是关于在计算机上从数据中产生“模型”（model）的算法，即“学习算法”（learning algorithm）。

  

# 1.2 基本术语

* 一组记录的集合称为一个“数据集”（data set），其中每条记录是关于一个事件或对象（如一个西瓜）的描述，称为一个“示例”（instance）或“样本”（sample）。

* 反应事件或对象在某方面的表现或性质的事项，例如“色泽”，“根蒂”，“敲声”。称为“属性”（attribute）或“特征”（feature）。

* 属性上的取值，例如“青绿”，“乌黑”，称为”属性值”（attribute value）。

* 属性张成的空间称为“属性空间”（attribute space）、“样本空间”（sample space）或者“输入空间”。

* 由于空间中的每个点对应一个坐标向量，因此我们也把一个示例称为一个“特征向量”（feature vector）。

* 一般地，令D = {$$x_1$$,$$x_2$$,...$$x_m$$}表示包含了m个示例的数据集，每个示例由d个属性描述，则每个示例$$x_i$$ = （$$x_{i1}$$;$$x_{i2}$$;...;$$x_{id}$$)是d维样本空间$$\chi$$中的一个向量，$$x_i\in\chi$$，其中$$x_{ij}$$是$$x_i$$在第j个属性上的取值，d称为样本$$x_i$$的“维数”（dimensionality）。

* 从数据中学得模型的过程称为“学习”（learning）或“训练”（training），这个过程通过执行某个学习 算法来完成。

* 训练过程中使用的数据称为“训练数据”（training data），其中 每个样本称为一个“训练样本”（training sample）。训练样本组成的集合称为 “训练集”（training set）。

* 学得模型对应了关于数据的某种潜在规律，因此亦称“假设”（hypothesis）。这种潜在规律自身，则称为“真相”或“真实”（ground-truth），学习 过程就是为了找出或逼近真相。

* 拥有了标记信息的示例，则称为“样例”（example）。一般地，用($$x_i,y_i$$)表示第i个样例，其中 $$y_i\in Y$$是 示例$$x_i$$的标记 ，Y是 所有标记的集合，，亦称“标记空间”（label space）或“输出空间”。

* 若我们欲预测的是离散值，例如“好瓜","坏瓜”，此类学习任务称为“分类”（classification）。

* 若欲预测的是连续值，例如西瓜成熟度0.95、0.37，此类学习任务称为“回归”（regression）。

* 学得模型后，使用其进行预测的过程称为“测试”（testing），被预测的样本称为“测试样本”（testing sample）。例如在学得 $$f$$后，对测试例$$x$$，可得到其预测标记$$y = f(x)$$。

* 我们还可以对西瓜做“聚类”（clustering），即将训练集中 的西瓜分成若干组，每组称为一个“簇”（cluster）；这样的学习过程有助于我们了解数据内在的规律，能为更深入地分析数据建立基础。

* 根据训练数据是否拥有标记信息，学习任务可大致划分为两大类：“监督学习”（supervised learning）和“无监督学习”（unsupervised learning），分类和回归是前者的代表，而聚类则是后者的代表。

* 机器学习的目标是使学得的模型能更好地适用于“新样本”，而不是仅仅在训练样本上工作得很好；即使对聚类这样的无监督学习任务，我们也希望学得的簇划分能适用于没在训练集中出现的样本。学得模型适用于新样本的能力，称为“泛化（generalization）能力”。

* 通常假设样本空间中全体样本服从一个未知“分布”（distribution）$$D$$,我们获得的每个样本都是独立地从这个分布上采样获得的，即“独立同分布”（independent and identically distribution，简称$$i.i.d$$）。一般而言，训练样本越多，为我们得到的关于$$D$$的信息越多，这样就越有可能学习获得具有强泛化能力的模型。

  

# 1.3 假设空间

* 广义的归纳学习大体相当于从样例中学习，而狭义 的归纳学习则要求从训练数据中心学得概念（concept），因此亦称为“概念学习”或“概念形成”。

* 我们可以把学习过程看作一个在所有假设（hypothesis）组成的空间中进行搜索的过程，搜索目标是找到与训练集“匹配”（fit）的 假设，即能够将训练集中的瓜判断正确的假设。

* 可以有许多策略对这个假设空间进行搜索，例如自顶向下、从一般到特殊，或是自底向上、从特殊到一般，搜索过程中可以不断删除与正例不一致的假设、和（或）与反例一致的假设。最终将获得与训练集一致（即对所有训练样本能够进行正确判断）的假设，这就是我们学得的结果 。

* 现实问题中我们常面临很大的假设空间，但学习过程是基于有限样本训练集进行的，因此，可能有多个假设与训练集一致，即存在着一个与训练集一致的“假设集合”，我们称之为“版本空间”（version space）。

  


# 1.4 归纳偏好

* 机器学习算法在学习过程中对某种类型假设的偏好，称为“归纳偏好”（inductive bias），或简称为“偏好”。
* 任何一个有效的机器学习算法必有其归纳偏好，否则它将被假设空间中看似在训练集上“等效”的假设所迷惑，而无法产生确定的学习结果。
* “奥卡姆剃刀”（Occam`s razor）是一种常用的、自然科学研究中心最基本的原则，即“若有多个假设与观察一致，则选最简单的那个”。
* 归纳偏好对应了学习算法本身所做出的关于“什么样的模型更好”的假设。在具体的现实问题中，这个假设是否成立，即算法的归纳偏好是否与问题本身匹配，大多数时候直接决定了算法能否取得好的性能。
* 无论学习算法$\varepsilon_a$多聪明，学习算法$\varepsilon_b$多笨拙，他们的期望性能竟然相同！这就是“没有免费的午餐”定理（No Free Lunch Theorem，简称NFL定理）
* NFL定理最重要的寓意，是让我们清楚地认识到，脱离具体问题，空泛地谈论“什么 学习算法更好”毫无意义，因为若考虑所有潜在的问题，则所有学习算法都一样好。要谈论算法的相对优劣，必须要针对具体的学习问题；在某些问题上表现好的学习算法，在另一些问题上却有可能不尽如人意，学习算法自身的归纳偏好与问题是否匹配，往往会起到决定性的作用。



# 1.5 发展历程

* 自行查阅。



# 1.6 应用现状

* 自行查阅。